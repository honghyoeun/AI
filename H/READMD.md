# 로봇 3원칙

- 1. 로봇은 인간을 해치면 안 된다 (A robot may not injure a human being or, through inaction, allow a human being to come to harm):

1.1 로봇이 인간에게 해를 가하거나 행동하지 않아서 인간이 해를 입는 것을 허용해서는 안 된다는 것을 강조합니다.

1.2 로봇 시스템은 안전성이 우선되어야 하며, 사람의 안전을 최우선으로 고려해야 합니다.

- 2. 로봇은 인간의 명령에 복종해야 하며, 그 명령에 위배되지 않아야 한다 (A robot must obey the orders given to it by human beings, except where such orders would conflict with the First Law):

2.1 로봇이 인간의 명령에 복종해야 하지만, 그 명령이 로봇이 인간에게 해를 가하거나 다른 원칙과 충돌할 경우에는 복종하지 않아야 한다는 것을 의미합니다.

2.2 로봇은 인간의 지시를 따르되, 안전과 윤리적인 가치를 준수해야 합니다.

- 3. 로봇은 자기를 보호할 수 있어야 하며, 그 방어가 인간에게 해를 가해서는 안 된다 (A robot must protect its own existence as long as such protection does not conflict with the First or Second Law):

3.1 로봇이 자신을 보호할 권리가 있으나, 이로 인해 인간에게 해를 가하거나 다른 두 원칙과 충돌하는 경우에는 자기를 보호하지 않아야 한다는 것을 나타냅니다.

3.2 로봇은 자체 안전을 유지하면서도 인간의 안전을 해치지 않도록 프로그래밍되어야 합니다.
